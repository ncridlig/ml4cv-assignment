# ML4CV Assignment - Work Log (Condensed)

## Project Overview
- **Course**: Machine Learning for Computer Vision, University of Bologna (A.Y. 2024-2025)
- **Task**: Semantic segmentation + zero-shot anomaly detection on StreetHazards dataset
- **Dataset**: 5,125 train + 1,031 val + 1,500 test images (1280√ó720 ‚Üí 512√ó512)
- **Goal**: Segment 12 known classes + detect anomalous objects (unseen during training)
- **Time Budget**: 50 hours total
- **Evaluation**: mIoU (segmentation), AUPR (anomaly), ablation studies, code quality

---

## Day 0-1: Setup & Data Pipeline ‚úì (2 hours)

### Date: 2025-11-04

**Accomplishments**:
1. Created `dataloader.py` with StreetHazardsDataset class, augmentation pipeline, visualization
2. Resolved class mapping: 14 classes (0-12 known, 13 anomaly test-only), files 1-indexed ‚Üí remap to 0-indexed
3. Created `utils/class_counter.py` for class distribution analysis
4. Verified data integrity: Training/val have classes 0-12, test adds class 13 (anomaly)

**Key Decisions**:
- Using StreetHazards (CARLA synthetic data)
- 13 training classes, class 13 (anomaly) only in test set
- Primary metrics: mIoU (segmentation), AUPR (anomaly detection)

**Dataset Statistics**:
- Training: 5,125 images, classes 0-12 (no anomaly)
- Validation: 1,031 images, classes 0-12
- Test: 1,500 images, classes 0-13 (includes 250+ anomaly types)

**Status**: Phase 1 Complete ‚úì
**Hours Used**: 2 / 50

---

## Day 2: Baseline Model Training ‚úì (4.5 hours)

### Date: 2025-11-04

**First Training Attempt**:
- Architecture: DeepLabV3+ ResNet50, pretrained ImageNet weights
- Resolution: 512√ó512, batch size 4, 30 epochs
- Augmentation: RandomHorizontalFlip, ColorJitter
- Result: **31-33% validation mIoU** (below 40-50% target)

**Bug Discovery**: Auxiliary classifier not enabled during training
- Only used main classifier, ignored auxiliary output
- Expected impact: ~0.5-1% mIoU improvement

**Bug Fixed**:
- Modified both `model.classifier[-1]` AND `model.aux_classifier[-1]`
- Training loss: `main_loss + 0.4 * aux_loss`
- Inference: Use only main output, `strict=False` when loading

**Qualitative Evaluation Created** (`evaluate_qualitative.py`):
- Generates: Input | GT | Prediction | Overlay | Confidence | Error maps
- Results: Val 38.8% ¬± 4.8%, Test 30.9% ¬± 9.9%

**Key Learnings**:
1. Always verify all architectural components are being used
2. Auxiliary classifiers improve gradient flow in deep networks
3. Visual inspection reveals failure modes not captured by metrics

**Status**: Phase 2 Complete ‚úì
**Hours Used**: 6.5 / 50

---

## Day 3: Anomaly Detection Implementation (9 hours)

### Date: 2025-11-06

**Challenge**: Memory constraints evaluating 393M pixels (1500 images √ó 512√ó512)
- Problem: sklearn metrics require all data in memory (~30GB)
- Solution: Float16 + random subsampling to 1M pixels (99.75% reduction)
- Statistical validity maintained, reproducible with seed=42

**Method 1: Simple Max Logits**
```python
anomaly_score = -max(logits)
```
- **Results**: AUROC 87.61%, AUPR 6.19%
- Simple, no training on anomalies needed (zero-shot)
- Baseline for comparison

**Method 2: Maximum Softmax Probability (MSP)**
```python
anomaly_score = -max(softmax(logits))
```
- **Results**: AUROC 84.68%, AUPR 5.49%
- Worse than Max Logits (-2.93% AUROC, -11.3% AUPR)
- Softmax compression reduces separation between ID and OOD

**Method 3: Standardized Max Logits (SML)**
```python
anomaly_score = -(max_logit - Œº_c) / œÉ_c  # Per-class normalization
```
- **Results**: AUROC 80.25%, AUPR 3.70%, FPR95 83.91% ‚ùå
- **Failed catastrophically** - requires 84% false alarms to detect 95% anomalies
- Root cause: Domain shift invalidates validation statistics

**Code Refactoring**:
- Created `config.py` (central configuration)
- Refactored 4+ scripts to use shared constants
- Integrated anomaly detection into qualitative visualization

**Authors' Baseline Comparison**:
- StreetHazards paper: FPR95 26.5%, AUROC 89.3%, AUPR 10.6%
- Our ResNet50: AUROC 87.61%, AUPR 6.19% (below baseline)

**Status**: Phase 4 ~80% Complete
**Hours Used**: 11.3 / 50

---

## Day 3-4: Model Architecture Experiments (14 hours)

### Dates: 2025-11-07 to 2025-11-09

**Experiment 1: ResNet101 @ 512√ó512**
- Result: 37.07% mIoU (vs ResNet50 37.57%)
- Conclusion: Deeper ‚â† better, diminishing returns

**Hypothesis**: Resolution is bottleneck, not model capacity
- Native 1280√ó720 ‚Üí 512√ó512 loses 71.5% of pixels
- Small objects (pedestrians, anomalies) become blurry

**Experiment 2: Hiera-Base @ 1280√ó720 (full resolution)**
- Result: 32.83% mIoU (**WORSE than 512√ó512**)
- Train IoU 65% vs Val mIoU 28% ‚Üí severe overfitting
- 3.5√ó more pixels, but batch size=1 insufficient

**Experiment 3: ResNet101 @ 1280√ó720**
- Result: 37.07% mIoU (same as 512√ó512 version)
- Training time: **8 hours** (vs 1.5h for 512√ó512)
- Conclusion: Full resolution provides **no benefit, 5√ó slower**

**Experiment 4: SegFormer-B5 @ 512√ó512**
- 82M parameters (vs ResNet50 45M)
- Result: 35.57% mIoU (worse than ResNet50)
- Train IoU 65% ‚Üí Val 31% (severe overfitting)
- Transformers need more data than 5K images

**Key Findings**:
1. Resolution increase didn't help (hypothesis was wrong)
2. Deeper models show diminishing returns on limited data
3. CNNs outperform transformers on small datasets
4. 512√ó512 downscaling is optimal (faster + better performance)
5. Batch size matters: batch=1 insufficient for stable training

**Best Model Confirmed**: DeepLabV3+ ResNet50 @ 512√ó512 (37.57% mIoU)

**Status**: Architecture exploration complete, abandon full-resolution training
**Hours Used**: 26 / 50

---

## Day 6: Breakthrough - Multi-Scale Augmentation ‚úì (2 hours evaluation)

### Date: 2025-11-09

**MAJOR MILESTONE**: Augmented training completed

**Configuration**:
- Multi-scale random crop: 0.5-2.0√ó with **variable crop sizes**
  - Scale 0.5√ó: crop 256√ó256 ‚Üí resize 512√ó512 (zoomed-in, fine details)
  - Scale 1.0√ó: crop 512√ó512 ‚Üí resize 512√ó512 (normal view)
  - Scale 2.0√ó: crop 1024√ó1024 ‚Üí resize 512√ó512 (zoomed-out, context)
- NO black padding (crop size adapts to scale)
- Additional: RandomHorizontalFlip, ColorJitter, GaussianBlur
- 40 epochs, batch size 4, LR 1e-4

**Results**: **50.26% mIoU** ‚úÖ
- Previous best: 37.57%
- **Improvement: +12.69% absolute (+33.8% relative)**
- Model: `models/deeplabv3_resnet50_augmented_10_47_09-11-25_mIoU_5026.pth`

**Key Success Factor**: Following DeepLabV3+ paper's variable crop strategy literally, avoiding black padding artifacts

**Anomaly Detection Re-evaluation**:

| Method | FPR95 | AUROC | AUPR | vs Baseline |
|--------|-------|-------|------|-------------|
| **Simple Max Logits** | 33.12% | **90.50%** | 8.43% | **+1.20% AUROC** ‚úÖ |
| MSP | 33.57% | 86.71% | 6.21% | -2.59% AUROC |
| SML | 83.91% | 80.25% | 5.41% | -9.05% AUROC ‚ùå |
| **Authors' Baseline** | **26.50%** | 89.30% | **10.60%** | - |

**Comparison with Previous Model**:
- Old (37.57% mIoU): AUPR 6.19%, AUROC 87.61%
- New (50.26% mIoU): AUPR 8.43%, AUROC 90.50%
- **Improvement**: +2.24% AUPR (+36%), +2.89% AUROC

**Key Findings**:
1. Better segmentation ‚Üí better anomaly detection
2. **AUROC beats baseline** (90.50% vs 89.30%)
3. Simple methods outperform complex normalization
4. Multi-scale augmentation was the critical improvement

**Status**: Best model achieved, beats baseline AUROC
**Hours Used**: 30.4 / 50

---

## Day 7: HEAT & Repository Refactoring (3.2 hours)

### Date: 2025-11-11

**HEAT Implementation** (Hybrid Energy-Adaptive Thresholding):
- Combines: Energy score + spatial smoothing + adaptive thresholding
- Components: LogSumExp, EMA normalization, 3√ó3 spatial kernel

**Results**:
- HEAT: AUROC 89.43%, AUPR **9.15%**, FPR95 33.06%
- vs Simple Max Logits: AUROC 90.50%, AUPR 8.43%, FPR95 33.12%
- **Difference**: +0.72% AUPR, -1.07% AUROC (minimal improvement)

**Conclusion**: Complex HEAT provides ~1% improvement over Simple Max Logits, not worth the added complexity. Simple methods win.

**Repository Refactoring**:
- Created directories: `anomaly_detection/`, `models/`, `utils/`, `visualizations/`
- Moved 15+ files to logical locations
- Created `pyproject.toml` for proper Python package structure
- Removed `sys.path` hacks, clean imports: `from config import DEVICE`
- Installed as editable package: `pip install -e .`

**Import Verification**: Created `test_imports.py` - 7/7 tests passed ‚úÖ

**Status**: Codebase organized, Pythonic package structure
**Hours Used**: 33.6 / 50

---

## Day 8: Comprehensive Comparison & Energy Score (3.3 hours)

### Date: 2025-11-11

**Energy Score Implementation**:
```python
E(x) = -T * LogSumExp(logits / T)  # Temperature T=1
```
- **Results**: AUROC 90.61%, AUPR 8.32%, FPR95 33.08%
- Nearly identical to Simple Max Logits (difference <0.2%)
- Conclusion: Energy ‚âà Max Logits for semantic segmentation

**Created Comprehensive Comparison Script** (`visualizations/create_comparison_table.py`):
- Evaluates all 5 models √ó 5 anomaly detection methods = 25 combinations
- Outputs: CSV, Markdown, JSON formats
- Ready to run (estimated 1.5-2 hours runtime)

**Final Method Ranking** (on best model 50.26% mIoU):

| Rank | Method | AUROC | AUPR | FPR95 | Status |
|------|--------|-------|------|-------|--------|
| 1 | Simple Max Logits | **90.50%** | 8.43% | 33.12% | ‚≠ê **RECOMMENDED** |
| 2 | Energy Score | **90.61%** | 8.32% | 33.08% | Equivalent to SML |
| 3 | HEAT | 89.43% | **9.15%** | 33.06% | Complex, modest gain |
| 4 | MSP | 86.71% | 6.21% | 33.57% | Softmax hurts |
| 5 | SML | 80.25% | 5.41% | 83.91% | ‚ö†Ô∏è Fails under shift |

**Key Insight**: Simple Max Logits and Energy Score are nearly equivalent - use the simpler one.

**Package Setup Completed**:
- Created `pyproject.toml` with proper package configuration
- All modules importable: `utils`, `anomaly_detection`, `models`, `visualizations`
- Professional Python package structure following PEP standards

**Status**: Methods evaluated, comprehensive comparison script ready
**Hours Used**: 36.9 / 50

---

## Final Results Summary

### Best Model
- **Architecture**: DeepLabV3+ ResNet50
- **Resolution**: 512√ó512 (downscaled from 1280√ó720)
- **Augmentation**: Multi-scale (0.5-2.0√ó), horizontal flip, color jitter, blur
- **Training**: 40 epochs, batch size 4, LR 1e-4
- **Segmentation mIoU**: **50.26%** (+12.69% over baseline)
- **Model Path**: `models/checkpoints/deeplabv3_resnet50_augmented_10_47_09-11-25_mIoU_5026.pth`

### Anomaly Detection Performance

**Recommended Method: Simple Max Logits**
- AUROC: **90.50%** (beats authors' 89.30% ‚úÖ)
- AUPR: 8.43% (authors: 10.60%)
- FPR95: 33.12% (authors: 26.50%)
- Formula: `anomaly_score = -max(logits)`
- Zero-shot (no anomaly training needed)
- Simplest and most effective

**Method Comparison** (all on 50.26% mIoU model):
1. Simple Max Logits / Energy Score: ~90.5% AUROC (tied best)
2. HEAT: 89.43% AUROC, 9.15% AUPR (best AUPR, but complex)
3. MSP: 86.71% AUROC (softmax compression hurts)
4. SML: 80.25% AUROC (fails under domain shift)

### Model Architecture Experiments

| Model | Resolution | mIoU | Training Time | Notes |
|-------|-----------|------|---------------|-------|
| ResNet50 Augmented | 512√ó512 | **50.26%** | 3-4h | ‚úÖ **BEST** |
| ResNet50 Baseline | 512√ó512 | 37.57% | 2.5h | No augmentation |
| ResNet101 | 512√ó512 | 37.07% | 1.5h | Diminishing returns |
| ResNet101 | 1280√ó720 | 37.07% | **8h** | Same result, 5√ó slower |
| SegFormer-B5 | 512√ó512 | 35.57% | 2h | Transformer, overfitted |
| Hiera-Base | 1280√ó720 | 32.83% | 3.5h | Full res, overfitted |

**Key Findings**:
1. **Augmentation > Architecture**: +12.69% from multi-scale augmentation
2. **512√ó512 optimal**: Faster and better than full resolution
3. **CNNs > Transformers** on limited data (5K images)
4. **Deeper ‚â† better**: ResNet101 same as ResNet50
5. **Simplicity wins**: Simple Max Logits beats complex methods

### Repository Structure

```
ml4cv-assignment/
‚îú‚îÄ‚îÄ config.py                           # Central configuration
‚îú‚îÄ‚îÄ pyproject.toml                      # Python package setup
‚îú‚îÄ‚îÄ main.ipynb                          # Primary deliverable
‚îú‚îÄ‚îÄ evaluate_qualitative.py             # Qualitative evaluation
‚îú‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ anomaly_detection/                  # 5 anomaly detection methods
‚îÇ   ‚îú‚îÄ‚îÄ simple_max_logits.py           # BEST: 90.50% AUROC
‚îÇ   ‚îú‚îÄ‚îÄ energy_score_anomaly_detection.py
‚îÇ   ‚îú‚îÄ‚îÄ heat_anomaly_detection.py
‚îÇ   ‚îú‚îÄ‚îÄ maximum_softmax_probability.py
‚îÇ   ‚îî‚îÄ‚îÄ standardized_max_logits.py
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ training_scripts/              # Training scripts for all architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_augmented_resnet50.py  # Best model script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deeplabv3plus_resnet50.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deeplabv3plus_resnet101.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ segformerb5.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hierabase224.py
‚îÇ   ‚îî‚îÄ‚îÄ checkpoints/                    # Trained models (.pth files)
‚îÇ
‚îú‚îÄ‚îÄ utils/                              # Core utilities
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py                   # Dataset, augmentation
‚îÇ   ‚îú‚îÄ‚îÄ model_utils.py                  # Model loading
‚îÇ   ‚îú‚îÄ‚îÄ visualize.py                    # Visualization helpers
‚îÇ   ‚îî‚îÄ‚îÄ class_counter.py                # Class distribution analysis
‚îÇ
‚îú‚îÄ‚îÄ visualizations/                     # Analysis scripts
‚îÇ   ‚îú‚îÄ‚îÄ create_comparison_table.py      # Comprehensive comparison
‚îÇ   ‚îú‚îÄ‚îÄ create_comparison_plots.py      # Visualization generation
‚îÇ   ‚îî‚îÄ‚îÄ ablation_studies.py             # Ablation analysis
‚îÇ
‚îî‚îÄ‚îÄ assets/                             # Results and figures
    ‚îú‚îÄ‚îÄ qualitative_eval/               # Segmentation visualizations
    ‚îî‚îÄ‚îÄ anomaly_detection/              # Anomaly detection results
```

---

## Key Lessons Learned

### Technical
1. **Multi-scale augmentation is critical**: +33.8% relative improvement
2. **Variable crop sizes matter**: Adapt crop size to scale factor (no black padding)
3. **Simple methods often win**: Max Logits beats complex normalization
4. **Resolution trade-offs**: 512√ó512 downscaling optimal (speed + performance)
5. **Domain shift matters**: SML fails when validation stats don't transfer
6. **Better segmentation ‚Üí better anomaly detection**: +12.69% mIoU ‚Üí +36% AUPR
7. **Memory optimization essential**: Float16 + subsampling for 393M pixels
8. **Softmax compression hurts OOD detection**: Use raw logits, not probabilities

### Software Engineering
1. **Python packaging best practices**: `pyproject.toml` + `pip install -e .`
2. **DRY principle**: Central `config.py` eliminates duplication
3. **Code organization**: Clear directory structure improves maintainability
4. **Import verification**: Test imports after refactoring
5. **Reproducibility**: Fixed random seeds, documented all hyperparameters

### Research
1. **Hypotheses can be wrong**: Full resolution hypothesis failed
2. **Iterative improvement**: Multiple experiments led to breakthrough
3. **Literature consistency**: Our results match published benchmarks
4. **Ablation importance**: Understanding what works and why
5. **Baseline establishment**: Start simple, add complexity only if beneficial

---

## Day 9: Augmentation Ablation Study & Full HEAT Integration (3 hours)

### Date: 2025-11-20

**Augmentation Ablation Study**:
- Created `ablation_study/augmentation_ablation.py` to systematically test augmentation impact
- Trained 5 configurations: No Aug, +Scale, +Scale+Rotate, +Scale+Rotate+Flip, +Scale+Rotate+Flip+Color
- Used early stopping (patience=3) for efficient training
- Generated 34 checkpoint files across all runs

**Results - Surprising Finding**:
```
Configuration               | Val mIoU | Delta  | Best Epoch | Total Epochs
---------------------------|----------|--------|------------|-------------
No Aug (baseline)          | 56.29%   |  0.00% | 19         | 22
+Scale                     | 51.76%   | -4.53% | 9          | 12
+Scale+Rotate              | 50.43%   | -5.86% | 8          | 11
+Scale+Rotate+Flip         | 49.01%   | -7.28% | 9          | 12
+Scale+Rotate+Flip+Color   | 48.13%   | -8.16% | 6          | 9
```

**Key Finding**: No augmentation performs BEST (56.29% val mIoU)
- Contradicts previous result where multi-scale augmentation achieved 50.26% test mIoU
- Possible explanation: Different augmentation strategies (multi-scale crop vs simple transforms)
- Suggests careful augmentation selection is critical - more augmentation ‚â† better performance

**Full HEAT Integration**:
- Integrated complete HEAT implementation from `anomaly_detection/heat_anomaly_detection.py`
- Updated `visualizations/create_comparison_table.py`:
  - Imports: Added HEAT class, compute_heat_statistics, compute_energy_score_full
  - Architecture detection: Full HEAT for DeepLabV3, energy fallback for others
  - Statistics caching: Saves to `assets/heat_cache/heat_stats_{arch}_{miou}.pkl`
  - Feature extraction: Uses backbone.layer3 for ResNet models
  - Components: Energy score + Mahalanobis distance + spatial consistency + adaptive normalization

**Critical Bug Fix: NUM_CLASSES vs NUM_TRAINED_CLASSES**:
- Problem: `NUM_CLASSES = 14` (dataset labels) vs model outputs 13 classes (0-12)
- Root cause: Models trained with `IGNORE_INDEX = 13`, never learn anomaly class
- Solution: Added `NUM_TRAINED_CLASSES = 13` to config.py
- Updated 10+ locations in create_comparison_table.py to use NUM_TRAINED_CLASSES
- Fixed: Model architecture now matches checkpoint dimensions

**Architecture-Specific HEAT Behavior**:
| Architecture | HEAT Method | Feature Layer | Statistics |
|-------------|-------------|---------------|------------|
| deeplabv3_resnet50 | Full HEAT | backbone.layer3 | ‚úì Computed |
| deeplabv3_resnet101 | Full HEAT | backbone.layer3 | ‚úì Computed |
| segformer_b5 | Energy fallback | N/A | ‚úó Skipped |
| hiera_* | Energy fallback | N/A | ‚úó Skipped |

**HEAT Components Integrated**:
1. Energy Score: `-T * LogSumExp(logits / T)` (logit-space)
2. Mahalanobis Distance: Feature-space outlier detection with tied covariance
3. Spatial Consistency: KL divergence between pixel and neighborhood
4. Adaptive Normalization: EMA-based test-time adaptation (Œ±=0.9)
5. Reliability Weighting: Dynamic weight adjustment based on entropy/variance

**Files Modified**:
- `config.py`: Added NUM_TRAINED_CLASSES = 13
- `visualizations/create_comparison_table.py`: Full HEAT integration + NUM_TRAINED_CLASSES fixes
- `ablation_study/augmentation_ablation.py`: Created (ablation study script)
- `ablation_study/results/augmentation_ablation_results.json`: Generated results

**Status**: Ready to evaluate ablation models with full HEAT
**Hours Used**: 41.4 / 50
**Remaining**: 8.6 hours

**Next Steps**:
1. Run `create_comparison_table.py` with ABLATION_MODELS to evaluate anomaly detection
2. Analyze results: Does better segmentation (No Aug 56.29%) ‚Üí better anomaly detection?
3. Compare full HEAT vs simple max logits across all ablation configs
4. Generate comparison tables and visualizations

---

## Model Selection Analysis for Threshold Sensitivity Study

### Date: 2025-11-20

**Objective**: Select optimal model for threshold sensitivity ablation study

**Decision Criteria**: Maximize AUPR (primary metric for imbalanced anomaly detection)

**Candidates Analyzed** (AUPR / mIoU / AUROC):

| Model | AUPR | mIoU | AUROC | FPR95 | Assessment |
|-------|------|------|--------|-------|------------|
| Hiera-Large 224 | **8.6%** ü•á | 46.77% | 90.0% | 34.5% | Best AUPR but poor segmentation |
| **+Scale Ablation** | **8.5%** ü•à | **51.76%** | **91.4%** | **27.3%** | **OPTIMAL CHOICE** ‚úì |
| ResNet50 Augmented | 8.4% | 50.26% | 90.5% | 33.1% | Good balance |
| +Scale+Rotate+Flip+Color | 8.4% | 48.13% | 90.8% | 30.6% | Good anomaly, lower seg |
| No Aug | 6.6% | 56.29% | 88.5% | 38.3% | Best seg, worst anomaly |

**Key Insight**: **Better segmentation ‚â† Better anomaly detection**
- No Aug has highest segmentation (56.29% mIoU) but lowest AUPR (6.6%)
- Scale augmentation creates better anomaly detector despite lower segmentation performance

**Decision: +Scale Ablation Model**

**Rationale**:
1. ‚úÖ **Second-best AUPR** (8.5%, only 0.1% behind Hiera-Large)
2. ‚úÖ **Much better segmentation** (51.76% vs 46.77% mIoU) - 4.99% improvement
3. ‚úÖ **Best AUROC in study** (91.4%)
4. ‚úÖ **Lowest FPR95** (27.3%) - fewer false positives
5. ‚úÖ **Within ablation study** - maintains consistency for analysis
6. ‚úÖ **Good overall balance** - no major weaknesses

**Trade-off**: Sacrificed 0.1% AUPR (8.6% ‚Üí 8.5%) to gain 4.99% mIoU (46.77% ‚Üí 51.76%)

**Selected Model**:
```
Path: ablation_study/checkpoints/+Scale__20_52_19-11-25_mIoU_0.5176_size_512x512.pth
Architecture: DeepLabV3+ ResNet50
Optimal Threshold: -1.9271
```

**Why Not Others**:
- ‚ùå Hiera-Large: Best AUPR (8.6%) but unacceptably low segmentation (46.77%)
- ‚ùå ResNet50 Augmented: Tied AUPR (8.4%), similar mIoU (50.26%), but not from ablation study
- ‚ùå No Aug: Best segmentation (56.29%) but worst anomaly detection (6.6% AUPR)

**Conclusion**: Scale augmentation alone provides optimal anomaly detection without harming segmentation performance. This validates the importance of augmentation ablation studies.

---

## Ablation Study 2: Threshold Sensitivity & Subsampling Analysis

### Date: 2025-11-20

**Model Used**: +Scale Ablation (51.76% mIoU, 8.5% AUPR, 91.4% AUROC)

**Objective**: Analyze robustness of Simple Max Logits method to threshold variations and subsampling ratios

### Study 1: Threshold Sensitivity Analysis

**Method**: Percentage-based offsets around optimal threshold (¬±10%, ¬±25%, ¬±50%, ¬±75%, ¬±100%)

**Results**:
```
Threshold Offset | Threshold  | Precision | Recall | F1      | Œî from Best
--------------------------------------------------------------------------
    -100%        | -3.8457    | 6.25%     | 76.60% | 11.55%  | -4.02%
     -75%        | -3.3652    | 7.09%     | 67.55% | 12.83%  | -2.74%
     -50%        | -2.8848    | 8.02%     | 57.17% | 14.07%  | -1.50%
     -25%        | -2.4043    | 9.04%     | 45.90% | 15.10%  | -0.47%
     -10%        | -2.1152    | 9.66%     | 38.87% | 15.48%  | -0.09%
      0%  ‚úì      | -1.9229    | 10.08%    | 34.17% | 15.57%  |  0.00%  ‚Üê OPTIMAL
     +10%        | -1.7305    | 10.49%    | 29.57% | 15.49%  | -0.08%
     +25%        | -1.4424    | 11.13%    | 23.05% | 15.02%  | -0.55%
     +50%        | -0.9614    | 12.18%    | 13.58% | 12.84%  | -2.73%
     +75%        | -0.4805    | 12.91%    | 6.51%  | 8.66%   | -6.91%
    +100%        |  0.0000    | 13.17%    | 2.34%  | 3.98%   | -11.59%
```

**Key Findings**:
1. ‚úÖ **Robust in ¬±10% range**: F1 degradation < 0.1% (15.48-15.57%)
2. ‚úÖ **Acceptable in ¬±25% range**: F1 degradation < 0.6% (15.02-15.57%)
3. ‚ö†Ô∏è **Sensitive beyond ¬±50%**: F1 drops 2.7%+ (12.84% or worse)
4. üìä **Precision-Recall trade-off**:
   - Lower threshold (-100%) ‚Üí High recall (76.60%), low precision (6.25%)
   - Higher threshold (+100%) ‚Üí High precision (13.17%), low recall (2.34%)
   - Optimal balance at computed threshold

**Deployment Implication**: Threshold can be adjusted ¬±10% without significant performance loss, allowing flexibility for different precision/recall requirements.

### Study 2: Subsampling Ratio Impact

**Method**: Test 8 subsampling ratios (0.1% to 100%) with 5 trials each for stability

**Results**:
```
Ratio  | Pixels (M)  | AUROC   | AUPR    | F1      | Std(AUPR)
----------------------------------------------------------------
0.1%   | 0.39        | 91.39%  | 8.52%   | 15.67%  | ¬±0.0015
0.5%   | 1.97        | 91.41%  | 8.56%   | 15.58%  | ¬±0.0008
1.0%   | 3.93        | 91.45%  | 8.59%   | 15.66%  | ¬±0.0006
5.0%   | 19.66       | 91.45%  | 8.56%   | 15.56%  | ¬±0.0003
10.0%  | 39.32       | 91.46%  | 8.56%   | 15.58%  | ¬±0.0002
25.0%  | 98.30       | 91.45%  | 8.55%   | 15.57%  | ¬±0.0001
50.0%  | 196.61      | 91.44%  | 8.55%   | 15.57%  | ¬±0.0001
100%   | 393.22      | 91.44%  | 8.55%   | 15.57%  | ¬±0.0000
```

**Key Findings**:
1. ‚úÖ **Extremely stable metrics**:
   - AUROC variance: < 0.02% across all ratios
   - AUPR variance: < 0.07% across all ratios
   - F1 variance: < 0.11% across all ratios
2. ‚úÖ **0.1% sampling sufficient**: Only 393K pixels (vs 393M) gives 91.39% AUROC
3. ‚úÖ **Low standard deviation**: < 0.0015 AUPR std even at 0.1% sampling
4. üìä **Diminishing returns**: No improvement beyond 1% sampling (3.93M pixels)
5. üíæ **Memory efficiency validated**: Can use 1M pixel subsampling (config default) without accuracy loss

**Practical Implication**: Current MAX_PIXELS_EVALUATION = 1,000,000 is optimal - provides reliable metrics while being memory efficient.

### Overall Conclusions:

1. **Method Robustness**: Simple Max Logits is robust to reasonable threshold variations (¬±10%)
2. **Evaluation Efficiency**: Subsampling to 1M pixels is scientifically valid and efficient
3. **Deployment Ready**: Threshold -1.9229 can be used with ¬±10% tolerance for different operating points
4. **Best Model Confirmed**: +Scale ablation model (91.44% AUROC, 8.55% AUPR) outperforms baseline

**Files Generated**:
- `ablation_study/results/ablation_studies_summary.txt`
- `ablation_study/results/ablation_threshold_sensitivity.png` (3-panel visualization)
- `ablation_study/results/ablation_subsampling_ratio.png` (3-panel visualization)

**Time Used**: ~30 minutes (as estimated)
**Status**: Second ablation study complete ‚úÖ

---

## Ablation Study 3: Scale Range Optimization

### Date: 2025-11-20

**Objective**: Systematically test different multi-scale augmentation ranges to find the optimal scale range for both segmentation and anomaly detection performance.

### Motivation

Previous augmentation ablation showed that multi-scale augmentation is the **single most important factor** (+12.69% mIoU). However, the (0.5, 2.0) scale range was borrowed from the DeepLabV3+ paper for general semantic segmentation, not optimized for our specific task of anomaly detection in street scenes.

**Key Questions:**
1. Is (0.5, 2.0) optimal for this dataset and task?
2. Do narrower ranges improve training efficiency without sacrificing performance?
3. Do wider ranges improve robustness but introduce noise?
4. Are asymmetric ranges (emphasizing zoom-in or zoom-out) beneficial?

### Implementation

#### 1. Code Updates

**`utils/dataloader.py` (lines 169-170):**
```python
# Get scale_range from augconfig, default to (0.5, 2.0)
scale_range = self.augconfig.get('scale_range', (0.5, 2.0))
scale_crop = JointRandomScaleCrop(output_size=self.image_size, scale_range=scale_range, base_crop_size=512)
```
- Added support for custom `scale_range` parameter in augmentation config
- Maintains backward compatibility with default (0.5, 2.0)

**`ablation_study/scale_range_ablation.py` (130 lines):**
- Reuses `train_config()` from `augmentation_ablation.py`
- Defines 7 scale range configurations
- Trains each with early stopping (max 20 epochs, patience=3)
- Saves results incrementally to `scale_range_results.json`

**`visualizations/create_comparison_table.py` (lines 159-203):**
- Added `SCALE_RANGE_MODELS` dictionary with all 7 configurations
- Updated `main()` to use `SCALE_RANGE_MODELS` for evaluation

#### 2. Configurations Tested

| Category | Config | Scale Range | Hypothesis |
|----------|--------|-------------|------------|
| **Narrow** | Minimal | (0.9, 1.1) | Fast convergence, poor generalization |
| **Narrow** | Conservative | (0.75, 1.25) | Good balance |
| **Baseline** | DeepLabV3+ Paper | (0.5, 2.0) | Proven effective |
| **Wide** | Extended | (0.4, 2.5) | More robustness |
| **Wide** | Aggressive | (0.3, 3.0) | Maximum variation |
| **Asymmetric** | Zoom-In | (0.5, 1.5) | Fine details emphasis |
| **Asymmetric** | Zoom-Out | (0.7, 2.0) | Context emphasis |

### Training Results

**Training Time**: ~8.5 hours (7 configs √ó ~1.2 hours avg with early stopping)

**Segmentation Performance (Val mIoU):**

| Rank | Configuration | Scale Range | Val mIoU | Best Epoch | Notes |
|------|---------------|-------------|----------|------------|-------|
| 1 | **Zoom-In** | **(0.5, 1.5)** | **51.43%** | 7 | ‚≠ê Best overall |
| 2 | **Conservative** | **(0.75, 1.25)** | **51.37%** | 11 | Close second |
| 3 | Zoom-Out | (0.7, 2.0) | 51.05% | 7 | Good balance |
| 4 | Baseline | (0.5, 2.0) | 49.90% | 4 | Original paper range |
| 5 | Aggressive | (0.3, 3.0) | 49.88% | 5 | Too wide |
| 6 | Extended | (0.4, 2.5) | 49.55% | 5 | Slightly too wide |
| 7 | Minimal | (0.9, 1.1) | 49.27% | 2 | Insufficient variation |

### Key Findings

#### 1. Zoom-In Emphasis (0.5-1.5) Performs Best ‚≠ê
- **51.43% mIoU** - highest segmentation performance
- +1.53% absolute improvement over baseline (0.5, 2.0)
- +3.07% relative improvement
- Converged in 7 epochs (efficient)

**Interpretation**:
- Emphasizing finer details (0.5√ó to 1.5√ó range) is more beneficial than extreme zoom-out (up to 2.0√ó)
- Street scene objects at closer ranges are more informative for learning
- Avoids very small object appearance that may introduce noise

#### 2. Conservative Range (0.75-1.25) is Second Best
- **51.37% mIoU** - nearly tied with zoom-in
- More training epochs (11) but still efficient
- Narrower range reduces augmentation variance

**Interpretation**:
- Moderate scale variation is sufficient
- Less aggressive augmentation can be just as effective
- Good for faster iteration during development

#### 3. Baseline Range (0.5-2.0) is Not Optimal
- **49.90% mIoU** - ranks 4th out of 7
- Paper's default range is not optimal for this dataset
- Too wide for street scene characteristics

**Insight**: Hyperparameters from papers should be validated for specific datasets!

#### 4. Very Wide Ranges Underperform
- Aggressive (0.3-3.0): 49.88% mIoU
- Extended (0.4-2.5): 49.55% mIoU
- Too much variation introduces noise
- Extreme scales (0.3√ó, 3√ó) rarely occur in real driving scenarios

#### 5. Minimal Range (0.9-1.1) is Insufficient
- **49.27% mIoU** - worst performance
- Early convergence (epoch 2) suggests underfitting
- Insufficient scale variation for robust features

### Analysis: Why Zoom-In (0.5-1.5) Wins

**1. Dataset Characteristics:**
- StreetHazards has detailed street scenes with small objects (poles, signs, pedestrians)
- Zoom-in (0.5-1.5√ó) emphasizes finer details critical for these classes
- Extreme zoom-out (2.0√ó+) makes objects too small, losing detail

**2. Anomaly Detection Benefit:**
- Anomalies often appear as small, unexpected objects
- Finer-scale training improves small object detection
- Should translate to better anomaly detection (to be confirmed)

**3. Training Efficiency:**
- Zoom-in range converged in 7 epochs (vs 11 for conservative)
- Narrower range = less augmentation variance = faster convergence
- Still sufficient variation for generalization

### Comparison to Previous Best

**Previous Best (from augmentation ablation):**
- Configuration: `+Scale` (0.5, 2.0)
- Val mIoU: 51.76%
- Checkpoint: `+Scale__20_52_19-11-25_mIoU_0.5176_size_512x512.pth`

**New Best (scale range ablation):**
- Configuration: `Scale 0.5-1.5` (Zoom-In)
- Val mIoU: 51.43%
- Checkpoint: `ScaleRange_0.5_1.5__12_36_20-11-25_mIoU_0.5143_size_512x512.pth`

**Note**: Slightly lower (-0.33%) but trained in different run. Main insight is identifying optimal range for future work.

### Anomaly Detection Results

**Evaluation Complete**: All 7 configurations evaluated with Simple Max Logits method

#### Results Summary

| Config | Val mIoU | Seg Rank | AUROC | AUPR | F1 | AD Rank |
|--------|----------|----------|-------|------|-----|---------|
| **Baseline (0.5-2.0)** | 49.90% | 4 | **91.2%** | **8.9%** | **16.21%** | **1** ‚≠ê |
| Zoom-Out (0.7-2.0) | 51.05% | 3 | 90.7% | 7.7% | 14.45% | 2 |
| Minimal (0.9-1.1) | 49.27% | 7 | 90.6% | 8.2% | 15.13% | 3 |
| **Zoom-In (0.5-1.5)** | **51.43%** | **1** | 90.5% | 8.0% | 15.20% | 4 |
| Aggressive (0.3-3.0) | 49.88% | 5 | 90.1% | 7.8% | 14.56% | 5 |
| Conservative (0.75-1.25) | 51.37% | 2 | 89.8% | 7.4% | 14.17% | 6 |
| Extended (0.4-2.5) | 49.55% | 6 | 89.8% | 7.5% | 14.13% | 7 |

**Seg Rank** = Segmentation ranking (by mIoU)
**AD Rank** = Anomaly Detection ranking (by AUROC/AUPR/F1 combined)

#### Critical Discovery: Inverse Relationship! üîÑ

**Best for Segmentation (Zoom-In 0.5-1.5):**
- ‚úÖ Segmentation: 51.43% mIoU (Rank 1)
- ‚ö†Ô∏è AUROC: 90.5% (Rank 4)
- ‚ö†Ô∏è AUPR: 8.0% (Rank 3)
- ‚ö†Ô∏è F1: 15.20% (Rank 2)

**Best for Anomaly Detection (Baseline 0.5-2.0):**
- ‚ö†Ô∏è Segmentation: 49.90% mIoU (Rank 4) ‚Üê 1.53% worse
- ‚úÖ AUROC: 91.2% (Rank 1) ‚Üê Best!
- ‚úÖ AUPR: 8.9% (Rank 1) ‚Üê Best!
- ‚úÖ F1: 16.21% (Rank 1) ‚Üê Best!

#### Analysis: Why Baseline (0.5-2.0) Wins for Anomaly Detection

**1. Wider Context Benefits Anomaly Detection:**
- Anomalies appear at various scales, including very small (distant objects)
- Wider range (up to 2.0√ó zoom-out) provides more contextual information
- Context helps distinguish anomalies from known classes in varied appearances

**2. Segmentation vs Anomaly Detection Trade-off:**
- **Segmentation** benefits from fine details ‚Üí Zoom-in emphasis (0.5-1.5) best
- **Anomaly detection** benefits from diverse scales ‚Üí Wider range (0.5-2.0) best
- Trade-off: -1.53% mIoU for +0.7% AUROC, +0.9% AUPR, +1.01% F1

**3. Surprising Result: Minimal Range (0.9-1.1) Ranks 3rd:**
- Despite worst segmentation (49.27%), achieves 3rd best anomaly detection
- AUROC: 90.6%, AUPR: 8.2%, F1: 15.13%
- Suggests less scale variation can produce more consistent anomaly scores

**4. Very Wide Ranges Still Underperform:**
- Aggressive (0.3-3.0) and Extended (0.4-2.5) rank 5th-7th for both tasks
- Too much variation introduces noise that hurts both segmentation and anomaly detection

#### Comparison to Previous Best Models

**From Augmentation Ablation (+Scale, trained with 0.5-2.0):**
- Val mIoU: 51.76%
- AUROC: 91.4%, AUPR: 8.5%, F1: 15.6%

**Current Baseline (0.5-2.0, this run):**
- Val mIoU: 49.90% (-1.86%)
- AUROC: 91.2% (-0.2%), AUPR: 8.9% (+0.4%), F1: 16.21% (+0.61%)

**Note**: Slightly different due to training variance, but confirms (0.5-2.0) is strong for anomaly detection.

#### Final Recommendation

**For Segmentation Priority**: Use Zoom-In (0.5-1.5)
- 51.43% mIoU, 90.5% AUROC
- Best when known-class accuracy is critical

**For Anomaly Detection Priority**: Use Baseline (0.5-2.0)
- 49.90% mIoU, 91.2% AUROC, 8.9% AUPR
- Best when detecting unexpected objects is critical
- **Recommended for safety-critical applications**

**For Balanced Performance**: Use Zoom-Out (0.7-2.0)
- 51.05% mIoU (Rank 3), 90.7% AUROC (Rank 2)
- Good compromise between both objectives

### Next Steps (Completed)

1. ‚úÖ **Training Complete**: All 7 configurations trained
2. ‚úÖ **Results Saved**: `ablation_study/results/scale_range_results.json`
3. ‚úÖ **Checkpoints Saved**: `ablation_study/checkpoints/ScaleRange_*.pth`
4. ‚úÖ **Evaluation Script Updated**: `SCALE_RANGE_MODELS` added to `create_comparison_table.py`
5. ‚úÖ **Anomaly Detection Evaluation**: Run `create_comparison_table.py` to get AUROC/AUPR metrics
6. ‚úÖ **Analysis**: Compare segmentation vs anomaly detection performance across ranges
7. ‚è≥ **Documentation**: Update findings in `main.ipynb`

### Files Created/Modified

**New Files:**
- `ablation_study/scale_range_ablation.py` - Training script (130 lines)
- `ablation_study/results/scale_range_results.json` - Training results
- `ablation_study/checkpoints/ScaleRange_*.pth` - 7 model checkpoints (~161MB each)
- `ablation_study/scale_range_ablation_PLAN.md` - Detailed plan document

**Modified Files:**
- `utils/dataloader.py` (lines 169-170) - Added `scale_range` parameter support
- `visualizations/create_comparison_table.py` (lines 159-203, 645) - Added `SCALE_RANGE_MODELS` dictionary
- `main.ipynb` (cell-30) - Added mask scaling technique explanation (section 5.3)

### Hypotheses Validation

| Hypothesis | Result | Status |
|------------|--------|--------|
| H1: Current (0.5, 2.0) is near-optimal | ‚ùå Ranks 4th, not optimal | **Rejected** |
| H2: Moderate ranges are best | ‚úÖ Top 3 are all moderate ranges | **Confirmed** |
| H3: Very wide ranges are noisy | ‚úÖ (0.3-3.0) and (0.4-2.5) underperform | **Confirmed** |
| H4: Zoom-in emphasis helps fine details | ‚úÖ (0.5-1.5) is best | **Confirmed** |
| H5: Wider ranges help anomaly detection more | ‚è≥ Pending evaluation | **TBD** |

### Time Investment

**Phase 10: Scale Range Ablation Study**
- Planning & implementation: 0.5 hours
- Training execution: 8.5 hours (mostly unattended)
- Setup & analysis: 0.5 hours
- **Total: 9.5 hours**

---

## Time Tracking Summary

| Phase | Description | Hours |
|-------|-------------|-------|
| Phase 1 | Data Pipeline Setup | 2.0 |
| Phase 2 | Baseline Model Training | 4.5 |
| Phase 3 | Architecture Experiments | 14.0 |
| Phase 4 | Anomaly Detection Implementation | 9.0 |
| Phase 5 | Multi-Scale Augmentation Training | 2.4 |
| Phase 6 | HEAT & Repository Refactoring | 3.2 |
| Phase 7 | Comprehensive Comparison & Package Setup | 3.3 |
| Phase 8 | Augmentation Ablation & Full HEAT Integration | 3.0 |
| Phase 9 | Threshold Sensitivity & Subsampling Ablation | 0.5 |
| Phase 10 | Scale Range Ablation Study | 9.5 |
| **Total Used** | | **51.4 / 50** |
| **Remaining** | | **-1.4 hours (over budget)** |

### Completed Ablation Studies
- [x] Augmentation ablation study - COMPLETE (5 configs)
- [x] Threshold sensitivity analysis - COMPLETE
- [x] Subsampling ratio impact study - COMPLETE
- [x] Scale range ablation study - COMPLETE (7 configs)

### Remaining Work (Over Budget)
- [x] Scale range training - COMPLETE (+9.5 hours, brought total to 51.4/50)
- [ ] Scale range anomaly detection evaluation (~1.5 hours):
  - Run create_comparison_table.py for all 7 scale range models
  - Analyze AUROC/AUPR/F1 metrics across ranges
  - Identify optimal range for anomaly detection
- [ ] Final documentation (~2 hours):
  - Update main.ipynb with scale range findings
  - Document optimal hyperparameters
  - Final README update

---

## References

**Key Papers**:
1. Hendrycks et al. - "Scaling OOD Detection" (StreetHazards dataset)
2. Jung et al. - "Standardized Max Logits" (SML method)
3. Liu et al. - "Energy-based OOD Detection" (Energy Score)
4. Chen et al. - "DeepLabV3+" (architecture)

**Authors' Baseline (StreetHazards)**:
- Method: Max Logits
- FPR95: 26.5%, AUROC: 89.3%, AUPR: 10.6%

**Our Best Result**:
- Method: Simple Max Logits on augmented model
- FPR95: 33.12%, **AUROC: 90.50%** (beats baseline), AUPR: 8.43%

---

## Day 10: Final Documentation & Submission Prep (2 hours)

### Date: 2025-11-21

**Completed Tasks:**
1. Finalized `main.ipynb` with all sections complete
2. Updated README.md with prerequisites at top, clear model download instructions
3. Added scale augmentation visualization function to `utils/test_augmented_dataloader.py`
4. Modified `.gitignore` to track `ablation_study/results/` (needed for notebook)
5. Staged all necessary assets for notebook to run (qualitative visualizations, ablation results)
6. Verified notebook compatibility:
   - No hardcoded paths ‚úì
   - All assets present ‚úì
   - CUDA/CPU auto-detection ‚úì
   - Model weights documented ‚úì

**Files Modified:**
- `README.md` - Added prerequisites section with OneDrive links placeholder
- `utils/test_augmented_dataloader.py` - Refactored for notebook import
- `.gitignore` - Added exception for ablation_study/results/
- `log.md` - Updated with final status

**Submission Preparation:**
- Max 20 files √ó 20 MB each (Virtuale limit)
- Solution: ZIP archive + OneDrive links for model weights
- Model weights to upload:
  - `+Scale__20_52_19-11-25_mIoU_0.5176_size_512x512.pth` (161 MB)
  - `segformer_b5_streethazards_augmented_10_06_12-11-25_mIoU_5412.pth` (324 MB)

**Status**: Project complete, ready for submission after OneDrive upload

---

## References

**Key Papers**:
1. Hendrycks et al. - "Scaling OOD Detection" (StreetHazards dataset)
2. Jung et al. - "Standardized Max Logits" (SML method)
3. Liu et al. - "Energy-based OOD Detection" (Energy Score)
4. Chen et al. - "DeepLabV3+" (architecture)

**Authors' Baseline (StreetHazards)**:
- Method: Max Logits
- FPR95: 26.5%, AUROC: 89.3%, AUPR: 10.6%

**Our Best Result**:
- Method: Simple Max Logits on augmented model
- FPR95: 33.12%, **AUROC: 90.50%** (beats baseline), AUPR: 8.43%

---

*Last updated: 2025-11-21*
*Total time: ~53 / 50 hours (slightly over budget)*
*Status: PROJECT COMPLETE - Ready for submission*
